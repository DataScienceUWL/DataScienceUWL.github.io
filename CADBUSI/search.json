[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer Aided Diagnosis for Breast Ultrasound Imagery (CADBUSI)",
    "section": "",
    "text": "A joint project between Mayo-La Crosse and University of Wisconsin-La Crosse. Supported by a seed grant from Mayo Health."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#project-goal",
    "href": "index.html#project-goal",
    "title": "Computer Aided Diagnosis for Breast Ultrasound",
    "section": "Project Goal",
    "text": "Project Goal\nProduce software that performs at least as well as human experts in the accurate interpretation and clinical assessment of breast lesions detected via ultrasound. The image below is a rough roadmap of our design.\n\n\n\nPipeline for Model Development\n\n\nMore details can be found in the project proposal and these presentation slides.\nOur focus is on building an unintrusive system that provides:\n\ndecision support focused on BIRADS 3 (no biopsy) and BIRADS 4 (biopsy required) lesions\nan overall score / probability of malignancy / BIRADS score\nexplainable results\n\nBIRADS characteristics\nheatmap overlay showing classification relevant parts of image\n\nan interactive mechanism for the radiologist to refine the diagnosis"
  },
  {
    "objectID": "investigators.html",
    "href": "investigators.html",
    "title": "Investigators",
    "section": "",
    "text": "Principal Investigators:\n\nDr. Jeff Baggett, Department of Mathematics and Statistics, UW - La Crosse\nDr. Song Chen, Department of Mathematics and Statistics, UW - La Crosse\nDr. Rich Ellis, Radiology and Imaging, Mayo - La Crosse\n\nGraduate Student Investigators:\n\nMasters of Science in Data Science at UWL:\n\nFlorin Andrei\nElsa Braun\nTeresa Bodart\nJustin Hall\nJosh Jarvey\nSuriya Mohan\nAdam Silverfein\nPranali Shendekar\n\n\nUndergraduate Student Investigators:\n\nComputer Science, Mathematics and Statistics, at UWL:\n\nTristan Hansen\nEric Jahns\nShuta Kumada\nSimon Wagner"
  },
  {
    "objectID": "Commercial.html",
    "href": "Commercial.html",
    "title": "Commercial Systems",
    "section": "",
    "text": "The paper “Updates in Artificial Intelligence for Breast Imaging” (Bahl 2022) reviews FDA approved software for multiple modalities."
  },
  {
    "objectID": "BUS_Commercial_CAD.html",
    "href": "BUS_Commercial_CAD.html",
    "title": "Commercial CAD Systems for Breast Imaging",
    "section": "",
    "text": "Koios DS Breast\n\nA video demo of DS Breast The 43 second video shows the user clicking (“Smart Click”) inside lesion in two views. We don’t know how those views were selected. The software produces a couple of BI-RADS descriptors, and a BI-RADS score. It also uses the user clicks to produce bounding boxes around the lesions.\n\nS-detect is a system built into some Samsung ultrasound machines. It attempts to determine BIRADS labels, segmention of lesion, and prediction of “normal”, “possibly benign”, or “possibly malignant”.\n\nThis paper builds a nomogram on top to the output of S-detect and incorporates patient age, radiologist’s BIRADS score, and outputs from S-detect to give a probability of malignancy.\nA video demo of S-Detect Requires user input of seed-point or oval surrounding suspected lesion. Produces BIRADS score and characteristics.\n\n\nSee-Mode is a Canadian-approved system that detects lesions and determines BIRADS labels.\n\nA MobiHealthNews article about See-Mode.\nA 10 minute presentation about See-Mode\n\n\nMindray\n\nA MobiHealthNews article about China’s Mindray.\nA video demo of Mindray Smart Breast It determines BIRADS labels. Not sure if it produces an overall BIRADS score.\n\n\n\n\n\nVolpara Breast Health Platform\n\nA MobiHealthNews article about Volpara.\n\n\nA MobiHealthNews article about Whiterabbit."
  },
  {
    "objectID": "index.html#goal",
    "href": "index.html#goal",
    "title": "Computer Aided Diagnosis for Breast Ultrasound Imagery (CADBUSI)",
    "section": "Goal",
    "text": "Goal\nProduce software that performs at least as well as human experts in the accurate interpretation and clinical assessment of breast lesions detected via ultrasound. The image below is a rough roadmap of our design.\n\n\n\nMap for CAD System Development\n\n\nMore details can be found in the project proposal and these presentation slides.\nOur focus is on building an unintrusive system that provides:\n\ndecision support focused on BIRADS 3 (no biopsy) and BIRADS 4 (biopsy required) lesions\nan overall score / probability of malignancy / BIRADS score\nexplainable results\n\nBIRADS characteristics\nheatmap overlay showing classification relevant parts of image\n\nan interactive mechanism for the radiologist to refine the diagnosis"
  },
  {
    "objectID": "Capstone Projects.html",
    "href": "Capstone Projects.html",
    "title": "Capstone Projects",
    "section": "",
    "text": "As of December 2022, six graduate students in the Masters of Science in Data Science program have completed capstone projects.\n\n\nAdam Silberfein, August 2021.Link to pdf file\nAbstract: Breast ultrasound has long been one the most preferred modalities of breast cancer detection due to its relatively low cost, ease of access, and noninvasive nature. Advancements in imaging technology and improvements in technique and standardization have led to corresponding increases in accuracy, but the potential for an additional boost exists from the application of artificial intelligence and machine learning. This paper explores how the potential of a quantitative model and associated machine learning prediction can assist radiologists currently using what is primarily a qualitative model to assign a BI-RADS score and characterize breast lesions as benign or malignant. The steps taken involve having an expert radiologist annotate a publicly available data set of 135 breast ultrasound images according to an established rubric, using image processing techniques to convert certain highly predictive features of that rubric to a numerical representation, and then feeding that quantitative representation into a machine learning model to predict malignancy of a validation set of images. The resulting model confirms that not only were lesion shape and circumscription important features for classification, but so was internal echotexture, which is much more difficult to differentiate to the naked eye. The model will be used to classify future larger data sets provided by our client, either in its current manifestation or after retraining on a subset of those new images.\n\n\n\nJustin Hall, December 2021. Link to pdf file\nAbstract: Breast ultrasounds are a standard modality used to find cancerous lesions and provide early care for patients. However, this modality is sensitive to user dependency. The discrepancies between radiologists’ readings for the same image may lead to inadequate patient care and inconsistent outcomes. As breast cancer continues to affect people around the globe, a new approach is needed to detect cancerous lesions. To provide a system for standardized classifications of these images, we examine the use of state-of-the-art convolutional neural network architectures to improve patient care. Our analysis showed that while all models showed similar AUC on our validation dataset of 121 images, DenseNet-201 maximized the F1-score. To provide confidence in our predictions, we then use local interpretable model-agnostic explanations (LIME) to find what image regions are essential to our model. Our model identified areas of lesion consistent with areas identified by trained radiologists suggesting that this model shows promise in aiding radiologists resulting in better care for patients.\n\n\n\nSuriya Mohan, May 2022, Link to pdf file\nAbstract: Breast cancer is the most frequently diagnosed cancer in women, and it is the leading cause of cancer-related deaths in women. Identifying the tumor in the early stage is essential to increase the survival rate of the patients. The breast ultrasound images combined with biopsy are clinically proven to be an effective method of diagnosing tumors. This project utilizes Artificial Intelligence (AI), such as deep learning models, to segment and classify the tumor in the ultrasound images. A segmentation deep learning model is utilized to detect the tumor in the ultrasound image, identify the region of interest (ROI), and draw a bounding box around the tumor. The radiologist confirms the ROI. Later the ROI is sent to the classification model to categorize cancer as benign or malignant. The radiologist can use the results from these models to make informed decisions, thereby reducing the manual errors involved in the current process. The AI technology assistance and radiologist expertise can lead to a process that is less prone to errors. The models were trained on augmented data set of 452 malignant images and 706 benign images. The accuracy of this model is 76.2%, precision 80%, and F1 score of 72.7%.\n\n\n\nJosh Jarvey, August 2022, Link to pdf file\nAbstract: According to the World Health Organization, breast cancer is now the most prevalent type of cancer diagnosed around the world. In 2020, it accounted for the deaths of roughly 685,000 women alone. Breast cancer incidence and mortality rates are expected to continue to rise in the coming decades, primarily in low to middle-income counties, due in part to an adoption of a more western lifestyle coupled with misconceptions about the nature and curability of the disease. Therefore, in partnership between the University of Wisconsin – La Crosse and Mayo Clinic Health Systems, the purpose of this study is to assess the feasibility of utilizing deep learning to aid radiologists with the interpretation of lesions discovered in breast ultrasound (BUS) images during routine clinical screenings. Based on a review of the literature on medical imaging and computer-assisted detection (CAD) systems for BUS interpretation, a multitask learning model using a pre-trained state-of-the-art convolutional neural network (CNN) was developed and trained using various image augmentation techniques known to increase performance. The research found that the best model identified from this study performed on par with that of a trained radiologist in its ability to predict lesion pathology. However, no definitive conclusions could be drawn about the model’s multitask performance due in part to the limited data available. Further research is needed as more data is made available from the Mayo Clinic, and alternative explainability methods may need to be explored.\n\n\n\nFlorin Andrei, December 2022, Link to pdf file\nAbstract: Breast cancer is the type of cancer with the highest prevalence nowadays, around the world. According to the World Health Organization, millions of cases are diagnosed each year, and in the year 2020 it has caused approximately 685,000 deaths worldwide. Forecasts indicate that the number of cases and the number of deaths may increase for the foreseeable future. Within a larger joint project by the University of Wisconsin - La Cross, and the Mayo Clinic Health Systems, this study has developed deep learning techniques that could be used to improve the diagnostic process for breast cancer. There are two main ways that deep learning technology could be applied: by providing real-time visual cues to radiologists while exploring lesions using ultrasound, and by creating predictions that become inputs for other models, in so-called ensemble methods, where multiple models work together to predict characteristics of the lesion. Based on a review of the literature on medical imaging and image segmentation techniques, several semantic segmentation models were created and trained on breast ultrasound imaging datasets. The research found that the best models performed at a level that would be expected from state-of-the-art image segmentation techniques, adjusted for the additional challenges raised by ultrasound imaging. Further work is needed within the parent project to integrate the models trained within this study with software usable by radiologists, and to explore the performance of ensemble methods where multiple models, including models trained in this study, work together to make predictions on ultrasound images that would ultimately lead to better diagnostic results and better patient outcomes.\n\n\n\nTeresa Bodart, December 2022, Link to pdf file\nAbstract: The International Agency for Research on Cancer announced that in 2020 female breast cancer became the most diagnosed cancer worldwide and the most common cause of cancer-related death in women. Still, breast cancer generally has a good prognosis with timely detection and appropriate treatment. Recently, computer-aided diagnosis (CAD) systems have shown promising results in using artificial intelligence (AI) to detect malignant lesions in breast ultrasound (US) imaging. When working with AI in a clinical setting, however, the American College of Radiology advocates for radiologist understanding of the algorithms in use. Accordingly, this study contributes to an ongoing collaboration between the University of Wisconsin-La Crosse and Mayo Clinic Enterprise by investigating three methods of AI explainability for the CAD software in development. Class activation maps, saliency maps, and attention map-enhanced class activation maps were compared to determine the most useful technique for visualizing regions in the US used by the models to determine pathology. The evaluation showed that saliency maps are the most promising method for visually explaining breast US classification. However, the small dataset and simplified model architecture used in this study mean that further research is necessary before fully implementing this method within the greater collaboration.\n\n\n\nElsa Braun, May 2023 Link to pdf file\nAbstract: Our company, Ultrasound.AI, is launching a hybrid deep learning algorithm. that will collaborate with radiologists in order to forecast the presence of breast cancer using ultrasound imagery. Our algorithm is unique in its ability to reduce radiologist time spent reviewing a patient’s images, thus improving throughput. Our algorithm can preview all images in a patient study and prioritize them for the radiologist. The Ultrasound.AI algorithm can match the accuracy of radiologists while reducing the need for unwarranted breast biopsies. Our algorithm uniquely offers transparency to the radiologist. A heatmap process allows the radiologist to measure how the algorithm arrived at its breast cancer prediction. With our next update, the Ultrasound.AI algorithm in collaboration with partner, Visage Imaging, will further save radiologist time by generating standardized diagnosis and treatment data (BI-RADS report) for the patient’s electronic medical record.\n\n\n\nPranali Shendekar, May 2023 Link to pdf file\nAbstract: This capstone project aimed to develop a software tool to manage and process ultrasound imaging data to train deep-learning models for predicting the presence of cancerous lesions in breast tissue. The project involved collecting, managing, updating, and summarizing study and annotation data in batches as they become available. The study data included ultrasound images, patient information, biopsy results, BI-RADS scores, and metadata about the images and ultrasound equipment. In addition, the annotation data had additional labels and visual outlines of the lesions, which were provided retrospectively by trained radiologists.\nThe software tool was designed to extract information from the metadata and text annotations accompanying each image and add it to the master index file. Additionally, the tool had to be able to copy the images to the correct locations in the collection and ensure that the image and patient IDs were distinct from those of previously added data. The tool also allowed for the possibility of overwriting existing data, adding additional columns to the index file, and incorporating corrections. Furthermore, the tool included additional columns that may be collected later from the annotation data.\nPython code was also developed to enable simple filtering of the image data and to write data downloaders allowing retrieving the image data from the collection. Finally, to show the efficiency of the database, the dataset was tested on a simple deep learning ResNet50 pre-trained model, and the PyTorch data loaders were used to pass data to the deep learning model.\nOverall, this project contributed to developing a useful tool that can aid in the early detection of breast cancer, leading to better patient outcomes. The software tool developed in this project can be used in further research studies in medical imaging, deep learning, and clinical settings to support radiologists in diagnosing breast cancer."
  },
  {
    "objectID": "Data Pipeline.html",
    "href": "Data Pipeline.html",
    "title": "Data Pipeline",
    "section": "",
    "text": "Below we describe our process for selecting, anonymizing, labeling, and preprocessing breast ultrasound images from Mayo so they may be used to train models locally at UW-La Crosse. It’s a rather cumbersome process and steps A, B, and C might be able to be done completely within Visage. Even step D might be launched from Visage\n\nStep A: - Data Acquisition and Deidentification (Datamart, Notion, Python)\n\nPart 1 – Identify relevant breast ultrasound studies\n\nDr. Ellis uses Datamart (Mayo Clinic/Radiology/Breast Imaging Section: Dashboards) to get spreadsheet that includes MRN and Accession Numbers for studies from patients with prior breast ultrasound and ultrasound-guided breast biopsy\n\nPart 2 – Use Notion to pull dicom files from database to Mayo workstation using breast ultrasound accession number\n\nPython is used to parse Datamart spreadsheet into multiple Notion query spreadsheets (<= 100 studies each)\nEach Notion query spreadsheet is manually uploaded to Notion.\n\nNotion pulls the dicom files and partially removes PHI from the dicom headers. (no pixel level de-id)\nManually download the partly deidentified dicom files from Notion to Mayo workstation\n\nPart 3 – Deidentification on Mayo workstation using Python to:\n\nclean dicom headers (saved locally as json files)\nblack out portion of image with PHI\nsave images locally as png files, videos are processed similarly\naccession numbers replaced with anonymized values and identification key saved locally as csv file\nnon PHI summary info saved to separate csv file\n\n\nStep B: Transfer Clinical Data from Mayo to UW-La Crosse\n\nbreast ultrasound images, BI-RADS scores, and breast biopsy pathologies are transfered as png and csv files\nno PHI is transfered\nonly anonymized data is shared via OneDrive\n\nStep C: Labeling (Python, Labelbox - see section at bottom of page)\n\nPart 1 – Most relevant images selection, to include best orthogonal views when possible\n\nLabelbox API and python used to upload studies for Dr. Ellis to choose best images\n\nPart 2 – Image Annotation and Segmentation\n\nMost relevant images combined locally into a single image panel which is uploaded to Labelbox queue\nDr. Ellis uses dropdowns and segmentation tools in Labelbox to annotate and segment (see sample image near the end of this page)\nSegmentation masks and annotation data downloaded from Labelbox\n\n\nStep D: Final Data Preparation\n\nPart 1 - Download masks and annotation from Labelbox\nPart 2 - Panels and annotations are processed locally using python. Including\n\nExtract pixel level text information about ultrasound probe orientation and left or right breast\nCrop the image to the largest rectangular region in the ultrasound part of the image\nUse a trained UNET segmentation model to identify and remove, via inpainting, pixel level annotations in the ultrasound image\nAdd filenames, anonymized patient IDs, resolution, pathology, probe orientation, left/right, etc. to database\n\nPart 3 - Images, masks, and labels are added to local database.\n\n\n\n\nWe upload a panel of 6 images to Labelbox. The two rows are the same images. The top row is used for segmentation (usually just the lesion is traced with a freehand tool), while the bottom row is maintained for reference. Dr. Ellis uses the drop down boxes to choose one label from each of the categories. In the future we want the radiologist to first select up to three images from each study: best two orthogonal views (without doppler) and best single doppler. The two non-doppler images will each need the lesion segmented and one or more labels selected within each of the categories. The doppler image will need only a single label.\nThe second row wouldn’t really be necessary if only a thin boundary for the segmentation mask was shown. The blue fill in Labelbox obscures the lesion too much to assess if the boundary is well drawn.\nHere is a sample:\n\n\n\nSample Labelbox Panel Image"
  },
  {
    "objectID": "Future Capstones.html",
    "href": "Future Capstones.html",
    "title": "Future Capstones",
    "section": "",
    "text": "The projects described below would be appropriate for satisfying the capstone requirements in the Masters of Science in Data Science Program. If you’re interested in working on one of these projects please contact Dr. Baggett (jbaggett at uwlax.edu)."
  },
  {
    "objectID": "Future Capstones.html#organize-and-develop-protocols-for-ultrasound-images-database",
    "href": "Future Capstones.html#organize-and-develop-protocols-for-ultrasound-images-database",
    "title": "Future Capstones",
    "section": "Organize and Develop Protocols for Ultrasound Images Database",
    "text": "Organize and Develop Protocols for Ultrasound Images Database\nWe have built a pipeline to de-identify, download, label, and process breast ultrasound images from Mayo. This data needs to be organized with a suitable way of storing all of the auxiliary information about each image/studies. Both the original de-identified images and their cropped and processed counterparts should be stored. A dynamic report (e.g. a notebook) should be developed that generates summary information each time the dataset is updated. Finally protocols for filtering the data should be developed. The report and filters will be similar to this paper about the NYU Breast Ultrasound Dataset."
  },
  {
    "objectID": "Future Capstones.html#develop-a-business-plan-for-build-cad-system-and-ui",
    "href": "Future Capstones.html#develop-a-business-plan-for-build-cad-system-and-ui",
    "title": "Future Capstones",
    "section": "Develop a Business Plan for build CAD System and UI",
    "text": "Develop a Business Plan for build CAD System and UI\nStudy competing products (for examples see this page) and interview radiologists to determine a UI design and roadmap for building Computer Aided Diagnosis software to support decisions about breast care based on breast ultrasound imagery. You’ll need to develop a broad understanding of what’s possible with current deep learning models, but you won’t have to implement the models in this project."
  },
  {
    "objectID": "Future Capstones.html#robustness-of-feature-extraction-for-downstream-modeling",
    "href": "Future Capstones.html#robustness-of-feature-extraction-for-downstream-modeling",
    "title": "Future Capstones",
    "section": "Robustness of Feature Extraction for Downstream Modeling",
    "text": "Robustness of Feature Extraction for Downstream Modeling\nGiven a breast ultrasound image that displays a lesion as well as a digital mask that shows which pixels are in the lesion we can extract quantitative and qualitative features from the image that are used in as predictors in a machine learning model for predicting malignancy. We have such a model that performs quite well, but to be useful we’ll need to automatically generate (segment) the lesion boundaries. We need to understand how the performance of the predictive model changes due to inaccuracies in the estimated boundary. To get an idea what this might look like see the paper “BIRADS Features-Oriented Semi-supervised Deep Learning for Breast Ultrasound Computer-Aided Diagnosis”. You can ask Dr. Baggett for a copy of this paper."
  },
  {
    "objectID": "Future Capstones.html#explore-semi-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "href": "Future Capstones.html#explore-semi-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "title": "Future Capstones",
    "section": "Explore Semi-Supervised Learning for Breast Ultrasound Deep Learning Models",
    "text": "Explore Semi-Supervised Learning for Breast Ultrasound Deep Learning Models\nFor all of the images/studies in our dataset we know the pathology of the lesion (benign or malignant) because the lesion has been biopsied. However there are many other characteristics of the lesion that we’d like to predict such as whether the boundary is smooth or irregular. Since labeling these characteristics requires much time and expertise it isn’t feasible to collect the labels for the entire dataset. In this project you’ll explore using semi-supervised learning to train deep learning models when only part of the dataset is labeled. Both convolutional neural network and transformer models should be trained."
  },
  {
    "objectID": "Future Capstones.html#explore-self-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "href": "Future Capstones.html#explore-self-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "title": "Future Capstones",
    "section": "Explore Self-Supervised Learning for Breast Ultrasound Deep Learning Models",
    "text": "Explore Self-Supervised Learning for Breast Ultrasound Deep Learning Models\nVision transformer models require large datasets to train from scratch and transfer learning may not work well if the target data (breast ultrasound images) is very different from the data used to train the model (natural images, Imagenet). Self-supervised learning replaces the classification task of the model with an alternative task so that the core of the model learns the features of the target data. Once the model is trained on a self-supervised task, the trained model is used to initialize the classification model. The idea for this project is to follow the approach in Is it Time to Replace CNNs with Transformers for Medical Images? but to apply it to breast ultrasound images."
  },
  {
    "objectID": "Future Capstones.html#refine-mult-task-model-for-predicting-malignancy-bi-rads-characteristics-and-bi-rads-score",
    "href": "Future Capstones.html#refine-mult-task-model-for-predicting-malignancy-bi-rads-characteristics-and-bi-rads-score",
    "title": "Future Capstones",
    "section": "Refine Mult-task Model for Predicting Malignancy, BI-RADS Characteristics, and BI-RADS score",
    "text": "Refine Mult-task Model for Predicting Malignancy, BI-RADS Characteristics, and BI-RADS score\nJosh Jarvey, an MSDS graduate, implemented a variation of the multitask model found in BI-RADS-Net: An Explainable Multitask Learning Approach for Cancer Diagnosis in Breast Ultrasound Images for his capstone project. The model should be refined and retrained using new data since the previous dataset was too small and incorrectly labeled."
  },
  {
    "objectID": "Future Capstones.html#refine-multi-task-model-for-predicting-malignancy-bi-rads-characteristics-and-bi-rads-score",
    "href": "Future Capstones.html#refine-multi-task-model-for-predicting-malignancy-bi-rads-characteristics-and-bi-rads-score",
    "title": "Future Capstones",
    "section": "Refine Multi-task Model for Predicting Malignancy, BI-RADS Characteristics, and BI-RADS score",
    "text": "Refine Multi-task Model for Predicting Malignancy, BI-RADS Characteristics, and BI-RADS score\nJosh Jarvey, an MSDS graduate, implemented a variation of the multitask model found in BI-RADS-Net: An Explainable Multitask Learning Approach for Cancer Diagnosis in Breast Ultrasound Images for his capstone project. The model should be refined and retrained using new data since the previous dataset was too small and incorrectly labeled. Since it’s unlikely that we’ll ever have a large dataset with complete BI-RADS labels it would be helpful to apply semi-supervised learning to training this model."
  },
  {
    "objectID": "Commercial.html#breast-ultrasound-cad",
    "href": "Commercial.html#breast-ultrasound-cad",
    "title": "Commercial Systems",
    "section": "Breast Ultrasound CAD",
    "text": "Breast Ultrasound CAD\n\nKoios DS Breast\n\nA video demo of DS Breast The 43 second video shows the user clicking (“Smart Click”) inside lesion in two views. We don’t know how those views were selected. The software produces a couple of BI-RADS descriptors, and a BI-RADS score. It also uses the user clicks to produce bounding boxes around the lesions.\n\nS-detect is a system built into some Samsung ultrasound machines. It attempts to determine BIRADS labels, segmention of lesion, and prediction of “normal”, “possibly benign”, or “possibly malignant”.\n\nThis paper builds a nomogram on top to the output of S-detect and incorporates patient age, radiologist’s BIRADS score, and outputs from S-detect to give a probability of malignancy.\nA video demo of S-Detect Requires user input of seed-point or oval surrounding suspected lesion. Produces BIRADS score and characteristics.\n\n\nSee-Mode is a Canadian-approved system that detects lesions and determines BIRADS labels.\n\nA MobiHealthNews article about See-Mode.\nA 10 minute presentation about See-Mode\n\n\nMindray\n\nA MobiHealthNews article about China’s Mindray.\nA video demo of Mindray Smart Breast It determines BIRADS labels. Not sure if it produces an overall BIRADS score.\n\n\n\nMammography CAD\n\nVolpara Breast Health Platform\n\nA MobiHealthNews article about Volpara.\n\n\nA MobiHealthNews article about Whiterabbit."
  },
  {
    "objectID": "SOTA_Reusults.html",
    "href": "SOTA_Reusults.html",
    "title": "SOTA Results",
    "section": "",
    "text": "Datasets:\nBUSI - three types normal, benign, malignant + masks BUSIS - 563 images with masks UDIAT - 163 images aka Dataset B OASBUD - small dataset in Matlab format with masks BUV - breast ultrasound videos bibliography: references.bib\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcc\nAUC\nSens.\nSpec.\nPrec.\nF1\nArch.\nData.\nYear\nRef\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.94\n\n0.96\n0.90\n0.92\n0.94\nAttention - VGG16\n\n\n[@kalafi_classification_nodate]"
  },
  {
    "objectID": "SOTA_Results.html",
    "href": "SOTA_Results.html",
    "title": "SOTA Results",
    "section": "",
    "text": "Datasets:"
  },
  {
    "objectID": "SOTA_Results.html#classification-results",
    "href": "SOTA_Results.html#classification-results",
    "title": "SOTA Results",
    "section": "Classification Results",
    "text": "Classification Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcc\nAUC\nSens.\nSpec.\nPrec.\nF1\nArch.\nData.\nYear\nRef\n\n\n\n\n.891\n0.962\n0.923\n0.831\n\n0.918\nResnet50\nprivate\n2022\nGuo et al. (2022)\n\n\n0.867\n0.95\n\n\n\n\nViTb32\nUDIAT + BUSI\n2022\nGheflati and Rivaz (2022)\n\n\n0.85\n0.94\n\n\n\n\nResnet50\nUDIAT + BUSI\n2022\nGheflati and Rivaz (2022)\n\n\n0.93\n\n0.96\n0.90\n0.92\n0.94\nAttention - VGG16\nUDIAT + Private\n2021\n(Kalafi et al., n.d.)"
  },
  {
    "objectID": "Capstone_Projects.html",
    "href": "Capstone_Projects.html",
    "title": "Capstone Projects",
    "section": "",
    "text": "As of December 2022, six graduate students in the Masters of Science in Data Science program have completed capstone projects.\n\n\nAdam Silberfein, August 2021.Link to pdf file\nAbstract: Breast ultrasound has long been one the most preferred modalities of breast cancer detection due to its relatively low cost, ease of access, and noninvasive nature. Advancements in imaging technology and improvements in technique and standardization have led to corresponding increases in accuracy, but the potential for an additional boost exists from the application of artificial intelligence and machine learning. This paper explores how the potential of a quantitative model and associated machine learning prediction can assist radiologists currently using what is primarily a qualitative model to assign a BI-RADS score and characterize breast lesions as benign or malignant. The steps taken involve having an expert radiologist annotate a publicly available data set of 135 breast ultrasound images according to an established rubric, using image processing techniques to convert certain highly predictive features of that rubric to a numerical representation, and then feeding that quantitative representation into a machine learning model to predict malignancy of a validation set of images. The resulting model confirms that not only were lesion shape and circumscription important features for classification, but so was internal echotexture, which is much more difficult to differentiate to the naked eye. The model will be used to classify future larger data sets provided by our client, either in its current manifestation or after retraining on a subset of those new images.\n\n\n\nJustin Hall, December 2021. Link to pdf file\nAbstract: Breast ultrasounds are a standard modality used to find cancerous lesions and provide early care for patients. However, this modality is sensitive to user dependency. The discrepancies between radiologists’ readings for the same image may lead to inadequate patient care and inconsistent outcomes. As breast cancer continues to affect people around the globe, a new approach is needed to detect cancerous lesions. To provide a system for standardized classifications of these images, we examine the use of state-of-the-art convolutional neural network architectures to improve patient care. Our analysis showed that while all models showed similar AUC on our validation dataset of 121 images, DenseNet-201 maximized the F1-score. To provide confidence in our predictions, we then use local interpretable model-agnostic explanations (LIME) to find what image regions are essential to our model. Our model identified areas of lesion consistent with areas identified by trained radiologists suggesting that this model shows promise in aiding radiologists resulting in better care for patients.\n\n\n\nSuriya Mohan, May 2022, Link to pdf file\nAbstract: Breast cancer is the most frequently diagnosed cancer in women, and it is the leading cause of cancer-related deaths in women. Identifying the tumor in the early stage is essential to increase the survival rate of the patients. The breast ultrasound images combined with biopsy are clinically proven to be an effective method of diagnosing tumors. This project utilizes Artificial Intelligence (AI), such as deep learning models, to segment and classify the tumor in the ultrasound images. A segmentation deep learning model is utilized to detect the tumor in the ultrasound image, identify the region of interest (ROI), and draw a bounding box around the tumor. The radiologist confirms the ROI. Later the ROI is sent to the classification model to categorize cancer as benign or malignant. The radiologist can use the results from these models to make informed decisions, thereby reducing the manual errors involved in the current process. The AI technology assistance and radiologist expertise can lead to a process that is less prone to errors. The models were trained on augmented data set of 452 malignant images and 706 benign images. The accuracy of this model is 76.2%, precision 80%, and F1 score of 72.7%.\n\n\n\nJosh Jarvey, August 2022, Link to pdf file\nAbstract: According to the World Health Organization, breast cancer is now the most prevalent type of cancer diagnosed around the world. In 2020, it accounted for the deaths of roughly 685,000 women alone. Breast cancer incidence and mortality rates are expected to continue to rise in the coming decades, primarily in low to middle-income counties, due in part to an adoption of a more western lifestyle coupled with misconceptions about the nature and curability of the disease. Therefore, in partnership between the University of Wisconsin – La Crosse and Mayo Clinic Health Systems, the purpose of this study is to assess the feasibility of utilizing deep learning to aid radiologists with the interpretation of lesions discovered in breast ultrasound (BUS) images during routine clinical screenings. Based on a review of the literature on medical imaging and computer-assisted detection (CAD) systems for BUS interpretation, a multitask learning model using a pre-trained state-of-the-art convolutional neural network (CNN) was developed and trained using various image augmentation techniques known to increase performance. The research found that the best model identified from this study performed on par with that of a trained radiologist in its ability to predict lesion pathology. However, no definitive conclusions could be drawn about the model’s multitask performance due in part to the limited data available. Further research is needed as more data is made available from the Mayo Clinic, and alternative explainability methods may need to be explored.\n\n\n\nFlorin Andrei, December 2022, Link to pdf file\nAbstract: Breast cancer is the type of cancer with the highest prevalence nowadays, around the world. According to the World Health Organization, millions of cases are diagnosed each year, and in the year 2020 it has caused approximately 685,000 deaths worldwide. Forecasts indicate that the number of cases and the number of deaths may increase for the foreseeable future. Within a larger joint project by the University of Wisconsin - La Cross, and the Mayo Clinic Health Systems, this study has developed deep learning techniques that could be used to improve the diagnostic process for breast cancer. There are two main ways that deep learning technology could be applied: by providing real-time visual cues to radiologists while exploring lesions using ultrasound, and by creating predictions that become inputs for other models, in so-called ensemble methods, where multiple models work together to predict characteristics of the lesion. Based on a review of the literature on medical imaging and image segmentation techniques, several semantic segmentation models were created and trained on breast ultrasound imaging datasets. The research found that the best models performed at a level that would be expected from state-of-the-art image segmentation techniques, adjusted for the additional challenges raised by ultrasound imaging. Further work is needed within the parent project to integrate the models trained within this study with software usable by radiologists, and to explore the performance of ensemble methods where multiple models, including models trained in this study, work together to make predictions on ultrasound images that would ultimately lead to better diagnostic results and better patient outcomes.\n\n\n\nTeresa Bodart, December 2022, Link to pdf file\nAbstract: The International Agency for Research on Cancer announced that in 2020 female breast cancer became the most diagnosed cancer worldwide and the most common cause of cancer-related death in women. Still, breast cancer generally has a good prognosis with timely detection and appropriate treatment. Recently, computer-aided diagnosis (CAD) systems have shown promising results in using artificial intelligence (AI) to detect malignant lesions in breast ultrasound (US) imaging. When working with AI in a clinical setting, however, the American College of Radiology advocates for radiologist understanding of the algorithms in use. Accordingly, this study contributes to an ongoing collaboration between the University of Wisconsin-La Crosse and Mayo Clinic Enterprise by investigating three methods of AI explainability for the CAD software in development. Class activation maps, saliency maps, and attention map-enhanced class activation maps were compared to determine the most useful technique for visualizing regions in the US used by the models to determine pathology. The evaluation showed that saliency maps are the most promising method for visually explaining breast US classification. However, the small dataset and simplified model architecture used in this study mean that further research is necessary before fully implementing this method within the greater collaboration.\n\n\n\nElsa Braun, May 2023 Link to pdf file\nAbstract: Our company, Ultrasound.AI, is launching a hybrid deep learning algorithm. that will collaborate with radiologists in order to forecast the presence of breast cancer using ultrasound imagery. Our algorithm is unique in its ability to reduce radiologist time spent reviewing a patient’s images, thus improving throughput. Our algorithm can preview all images in a patient study and prioritize them for the radiologist. The Ultrasound.AI algorithm can match the accuracy of radiologists while reducing the need for unwarranted breast biopsies. Our algorithm uniquely offers transparency to the radiologist. A heatmap process allows the radiologist to measure how the algorithm arrived at its breast cancer prediction. With our next update, the Ultrasound.AI algorithm in collaboration with partner, Visage Imaging, will further save radiologist time by generating standardized diagnosis and treatment data (BI-RADS report) for the patient’s electronic medical record.\n\n\n\nPranali Shendekar, May 2023 Link to pdf file\nAbstract: This capstone project aimed to develop a software tool to manage and process ultrasound imaging data to train deep-learning models for predicting the presence of cancerous lesions in breast tissue. The project involved collecting, managing, updating, and summarizing study and annotation data in batches as they become available. The study data included ultrasound images, patient information, biopsy results, BI-RADS scores, and metadata about the images and ultrasound equipment. In addition, the annotation data had additional labels and visual outlines of the lesions, which were provided retrospectively by trained radiologists.\nThe software tool was designed to extract information from the metadata and text annotations accompanying each image and add it to the master index file. Additionally, the tool had to be able to copy the images to the correct locations in the collection and ensure that the image and patient IDs were distinct from those of previously added data. The tool also allowed for the possibility of overwriting existing data, adding additional columns to the index file, and incorporating corrections. Furthermore, the tool included additional columns that may be collected later from the annotation data.\nPython code was also developed to enable simple filtering of the image data and to write data downloaders allowing retrieving the image data from the collection. Finally, to show the efficiency of the database, the dataset was tested on a simple deep learning ResNet50 pre-trained model, and the PyTorch data loaders were used to pass data to the deep learning model.\nOverall, this project contributed to developing a useful tool that can aid in the early detection of breast cancer, leading to better patient outcomes. The software tool developed in this project can be used in further research studies in medical imaging, deep learning, and clinical settings to support radiologists in diagnosing breast cancer."
  },
  {
    "objectID": "Data_Pipeline.html",
    "href": "Data_Pipeline.html",
    "title": "Data Pipeline",
    "section": "",
    "text": "Below we describe our process for selecting, anonymizing, labeling, and preprocessing breast ultrasound images from Mayo so they may be used to train models locally at UW-La Crosse. It’s a rather cumbersome process and steps A, B, and C might be able to be done completely within Visage. Even step D might be launched from Visage\n\nStep A: - Data Acquisition and Deidentification (Datamart, Notion, Python)\n\nPart 1 – Identify relevant breast ultrasound studies\n\nDr. Ellis uses Datamart (Mayo Clinic/Radiology/Breast Imaging Section: Dashboards) to get spreadsheet that includes MRN and Accession Numbers for studies from patients with prior breast ultrasound and ultrasound-guided breast biopsy\n\nPart 2 – Use Notion to pull dicom files from database to Mayo workstation using breast ultrasound accession number\n\nPython is used to parse Datamart spreadsheet into multiple Notion query spreadsheets (<= 100 studies each)\nEach Notion query spreadsheet is manually uploaded to Notion.\n\nNotion pulls the dicom files and partially removes PHI from the dicom headers. (no pixel level de-id)\nManually download the partly deidentified dicom files from Notion to Mayo workstation\n\nPart 3 – Deidentification on Mayo workstation using Python to:\n\nclean dicom headers (saved locally as json files)\nblack out portion of image with PHI\nsave images locally as png files, videos are processed similarly\naccession numbers replaced with anonymized values and identification key saved locally as csv file\nnon PHI summary info saved to separate csv file\n\n\nStep B: Transfer Clinical Data from Mayo to UW-La Crosse\n\nbreast ultrasound images, BI-RADS scores, and breast biopsy pathologies are transfered as png and csv files\nno PHI is transfered\nonly anonymized data is shared via OneDrive\n\nStep C: Labeling (Python, Labelbox - see section at bottom of page)\n\nPart 1 – Most relevant images selection, to include best orthogonal views when possible\n\nLabelbox API and python used to upload studies for Dr. Ellis to choose best images\n\nPart 2 – Image Annotation and Segmentation\n\nMost relevant images combined locally into a single image panel which is uploaded to Labelbox queue\nDr. Ellis uses dropdowns and segmentation tools in Labelbox to annotate and segment (see sample image near the end of this page)\nSegmentation masks and annotation data downloaded from Labelbox\n\n\nStep D: Final Data Preparation\n\nPart 1 - Download masks and annotation from Labelbox\nPart 2 - Panels and annotations are processed locally using python. Including\n\nExtract pixel level text information about ultrasound probe orientation and left or right breast\nCrop the image to the largest rectangular region in the ultrasound part of the image\nUse a trained UNET segmentation model to identify and remove, via inpainting, pixel level annotations in the ultrasound image\nAdd filenames, anonymized patient IDs, resolution, pathology, probe orientation, left/right, etc. to database\n\nPart 3 - Images, masks, and labels are added to local database.\n\n\n\n\nWe upload a panel of 6 images to Labelbox. The two rows are the same images. The top row is used for segmentation (usually just the lesion is traced with a freehand tool), while the bottom row is maintained for reference. Dr. Ellis uses the drop down boxes to choose one label from each of the categories. In the future we want the radiologist to first select up to three images from each study: best two orthogonal views (without doppler) and best single doppler. The two non-doppler images will each need the lesion segmented and one or more labels selected within each of the categories. The doppler image will need only a single label.\nThe second row wouldn’t really be necessary if only a thin boundary for the segmentation mask was shown. The blue fill in Labelbox obscures the lesion too much to assess if the boundary is well drawn.\nHere is a sample:\n\n\n\nSample Labelbox Panel Image"
  },
  {
    "objectID": "Future_Capstones.html",
    "href": "Future_Capstones.html",
    "title": "Future Capstones",
    "section": "",
    "text": "The projects described below would be appropriate for satisfying the capstone requirements in the Masters of Science in Data Science Program. If you’re interested in working on one of these projects please contact Dr. Baggett (jbaggett at uwlax.edu)."
  },
  {
    "objectID": "Future_Capstones.html#improving-breast-ultrasound-lesion-segmentation-with-generative-ai",
    "href": "Future_Capstones.html#improving-breast-ultrasound-lesion-segmentation-with-generative-ai",
    "title": "Future Capstones",
    "section": "Improving Breast Ultrasound Lesion Segmentation with Generative AI",
    "text": "Improving Breast Ultrasound Lesion Segmentation with Generative AI\nThe largest hurdle to training models for segmentation (tracing an outline) of lesions present in breast ultrasound images is obtaining adequate training data. It may be possible to augment existing training data by using generative AI to draw new backgrounds around already segmented lesions. One approach to doing this is described in this Medium article. In this project you would use stable diffision to augment segmentation training data for breast lesions in ultrasound imagery."
  },
  {
    "objectID": "Future_Capstones.html#using-self-supervision-to-pretrain-ai-models-for-lesion-detection-in-breast-ultrasound-images",
    "href": "Future_Capstones.html#using-self-supervision-to-pretrain-ai-models-for-lesion-detection-in-breast-ultrasound-images",
    "title": "Future Capstones",
    "section": "Using Self-Supervision to Pretrain AI Models for Lesion Detection in Breast Ultrasound Images",
    "text": "Using Self-Supervision to Pretrain AI Models for Lesion Detection in Breast Ultrasound Images\nGetting labeled breast ultrasound images is time consuming and expensive since it requires expert radiologists to do the labeling. Self-supervision is an unsupervised approach for pretraining deep-learning models. The backbone of the trained model can then be used to build classification and segmentation models that have been shown to perform nearly as well as full-supervised models. You would test this approach using breast ultrasound images. Related approaches are described in these papers Is it Time to Replace CNNs with Transformers for Medical Images? and Robust and Efficient Medical Imaging with Self-Supervision"
  },
  {
    "objectID": "Future_Capstones.html#robustness-of-feature-extraction-for-downstream-modeling",
    "href": "Future_Capstones.html#robustness-of-feature-extraction-for-downstream-modeling",
    "title": "Future Capstones",
    "section": "Robustness of Feature Extraction for Downstream Modeling",
    "text": "Robustness of Feature Extraction for Downstream Modeling\nGiven a breast ultrasound image that displays a lesion as well as a digital mask that shows which pixels are in the lesion we can extract quantitative and qualitative features from the image that are used in as predictors in a machine learning model for predicting malignancy. We have such a model that performs quite well, but to be useful we’ll need to automatically generate (segment) the lesion boundaries. We need to understand how the performance of the predictive model changes due to inaccuracies in the estimated boundary. To get an idea what this might look like see the paper “BIRADS Features-Oriented Semi-supervised Deep Learning for Breast Ultrasound Computer-Aided Diagnosis”. You can ask Dr. Baggett for a copy of this paper."
  },
  {
    "objectID": "Future_Capstones.html#explore-semi-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "href": "Future_Capstones.html#explore-semi-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "title": "Future Capstones",
    "section": "Explore Semi-Supervised Learning for Breast Ultrasound Deep Learning Models",
    "text": "Explore Semi-Supervised Learning for Breast Ultrasound Deep Learning Models\nFor all of the images/studies in our dataset we know the pathology of the lesion (benign or malignant) because the lesion has been biopsied. However there are many other characteristics of the lesion that we’d like to predict such as whether the boundary is smooth or irregular. Since labeling these characteristics requires much time and expertise it isn’t feasible to collect the labels for the entire dataset. In this project you’ll explore using semi-supervised learning to train deep learning models when only part of the dataset is labeled. Both convolutional neural network and transformer models should be trained."
  },
  {
    "objectID": "Future_Capstones.html#explore-self-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "href": "Future_Capstones.html#explore-self-supervised-learning-for-breast-ultrasound-deep-learning-models",
    "title": "Future Capstones",
    "section": "Explore Self-Supervised Learning for Breast Ultrasound Deep Learning Models",
    "text": "Explore Self-Supervised Learning for Breast Ultrasound Deep Learning Models\nVision transformer models require large datasets to train from scratch and transfer learning may not work well if the target data (breast ultrasound images) is very different from the data used to train the model (natural images, Imagenet). Self-supervised learning replaces the classification task of the model with an alternative task so that the core of the model learns the features of the target data. Once the model is trained on a self-supervised task, the trained model is used to initialize the classification model. The idea for this project is to follow the approach in Is it Time to Replace CNNs with Transformers for Medical Images? but to apply it to breast ultrasound images."
  },
  {
    "objectID": "Future_Capstones.html#refine-multi-task-model-for-predicting-malignancy-bi-rads-characteristics-and-bi-rads-score",
    "href": "Future_Capstones.html#refine-multi-task-model-for-predicting-malignancy-bi-rads-characteristics-and-bi-rads-score",
    "title": "Future Capstones",
    "section": "Refine Multi-task Model for Predicting Malignancy, BI-RADS Characteristics, and BI-RADS score",
    "text": "Refine Multi-task Model for Predicting Malignancy, BI-RADS Characteristics, and BI-RADS score\nJosh Jarvey, an MSDS graduate, implemented a variation of the multitask model found in BI-RADS-Net: An Explainable Multitask Learning Approach for Cancer Diagnosis in Breast Ultrasound Images for his capstone project. The model should be refined and retrained using new data since the previous dataset was too small and incorrectly labeled. Since it’s unlikely that we’ll ever have a large dataset with complete BI-RADS labels it would be helpful to apply semi-supervised learning to training this model."
  }
]