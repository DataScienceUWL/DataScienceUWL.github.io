<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CADBUSI - Capstone Projects</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar docked fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Capstone Projects</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CADBUSI</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Basics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Goal</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Background.html" class="sidebar-item-text sidebar-link">Background &amp; SOTA</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./investigators.html" class="sidebar-item-text sidebar-link">Investigators</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Accomplishments</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Data_Pipeline.html" class="sidebar-item-text sidebar-link">Data Pipeline</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Past_Capstones.html" class="sidebar-item-text sidebar-link active">Capstone Projects</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Background</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Commercial.html" class="sidebar-item-text sidebar-link">Commercial Systems</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Future Work</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Future_Capstones.html" class="sidebar-item-text sidebar-link">Future Capstones</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Reference</span>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Capstone Projects</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="bus-cad-completed-capstone-projects" class="level2">
<h2 class="anchored" data-anchor-id="bus-cad-completed-capstone-projects">BUS-CAD Completed Capstone Projects</h2>
<p>As of May 2024, eleven graduate students in the Masters of Science in Data Science program have completed capstone projects related to this research effort. The projects are organized chronologically starting with the latest:</p>
<section id="localization-effects-in-deep-learning-classification-of-breast-cancer-ultrasounds" class="level3">
<h3 class="anchored" data-anchor-id="localization-effects-in-deep-learning-classification-of-breast-cancer-ultrasounds">Localization Effects in Deep Learning Classification of Breast Cancer Ultrasounds</h3>
<p>Rachel Kim, May 2024 <a href="./documents/Capstones/Localization_Effects_Rachel_Kim.pdf">link to pdf file</a></p>
<p><strong>Abstract:</strong> Early detection of malignancy is a critical factor to effectively treating breast cancer. As radiologist determination of breast cancer ultrasound (BUS) images for malignancy is highly variable across providers, there is a need to develop computer-aided diagnosis (CAD) tools that can improve consistency and reliability of BUS diagnosis. Existing literature on the subject has shown promising results in the direction of creating a deep learning model to predict the malignancy of a BUS lesion. This study is a part of a larger ongoing research initiative between the University of Wisconsin-La Crosse and Mayo Clinic, where creating a CAD tool for BUS image classification to assist in radiologist diagnoses is the primary goal. For this study, various input images are proposed and explored to determine if adding localization information to the deep learning model inputs increases the model’s predictive power. The research hypothesis is that adding localization information will improve a model’s performance by highlighting the key diagnostic components to the deep learning model. Two pre-trained PyTorch models, ResNet18 and EfficientNetB3, were constructed and trained on three types of custom image inputs including localization information as well as a control of the original BUS image. A combination of private and public BUS images were used, totaling 3,784 images and associated binary masks. The results after training and testing the models on each input were that localization information in a custom input image does increase the predictive performance of a model, with further experimentation needed to identify which specific input is the optimal choice. These results suggest that incorporating localization into a CAD tool for BUS diagnosis could improve the process for both patients and hospitals.</p>
</section>
<section id="hover-trans-application-for-enhanced-breast-cancer-diagnosis-in-mayo-clinic-datasets" class="level3">
<h3 class="anchored" data-anchor-id="hover-trans-application-for-enhanced-breast-cancer-diagnosis-in-mayo-clinic-datasets">HoVer-Trans Application for Enhanced Breast Cancer Diagnosis in Mayo Clinic Datasets</h3>
<p>Clif Posey, May 2024 <a href="./documents/Capstones/HoVer-Trans_Clif_Posey.pdf">link to pdf file</a></p>
<p>Breast cancer is the most prevalent cancer in women globally. More than 2.26 million new cases of breast cancer have been recorded worldwide, and it is a significant cause of cancer-related deaths (World Cancer Research Fund International, 2020). Computer aided detection has shown promising results in accurate classification of malignant and benign tumors on breast ultrasound. This study aimed to apply the HoVer-Trans model to the Mayo Clinic breast ultrasound data and to test the model results compared to Jarvey (2022) and Mo et al.&nbsp;(2022). The results showed that the HoVer-Trans model demonstrated greater AUC, which could correctly classify breast lesions as benign or malignant. However, it under-performed in sensitivity compared to a radiologist’s results (Jarvey, 2022). Improved model optimization of the HoVer-Trans model could produce more precise and accurate results than this study’s results. Future research on AI in breast cancer diagnosis using machine learning enhances early detection, and rapid intervention for breast cancer is essential for patient care and optimal outcomes. Machine learning and AI can contribute significantly to diagnostic medicine in terms of breast cancer.</p>
</section>
<section id="attention-based-deep-multi-instance-learning-for-weakly-labeled-breast-ultrasound-image-classification" class="level3">
<h3 class="anchored" data-anchor-id="attention-based-deep-multi-instance-learning-for-weakly-labeled-breast-ultrasound-image-classification">Attention-Based Deep Multi-Instance Learning for Weakly-labeled Breast Ultrasound Image Classification</h3>
<p>Zena Fantaye, December 2023 <a href="./documents/Capstones/Attention-based_MIL_for_Breast_Ultrasound_Zenna_Fantaye.pdf">link to pdf file</a></p>
<p><strong>Abstract:</strong> This paper proposes using Attention-Based Multiple Instance Learning (ABMIL) on ultrasound images for breast cancer diagnosis. Rooted in an extensive literature review and building upon Shen et al.’s (2021) work, the study addresses challenges in traditional screening methods, particularly for cases with dense breast tissue. The paper provides a comprehensive overview of the research goals, literature review, methodology, and empirical findings. It thoroughly explores each aspect, giving insights into the details of the study. The study compiles and processes 40,000 ultrasound images from the Mayo Clinic using a selective methodology to enhance diagnostic accuracy. The ABMIL model produced promising results, achieving an AUC-ROC of 0.8021, sensitivity of 0.73, and specificity of 0.87. The model’s effectiveness with weakly labeled datasets is underscored by training and validation accuracy rates of 0.80 and 0.82, respectively. The model’s interpretability is prioritized through attention mechanisms, corroborated by successful model application to an external dataset (Imagenette). The research contributes to the evolving medical imaging landscape, exploring ABMIL’s potential to advance breast cancer diagnosis and improve patient care.</p>
</section>
<section id="organizing-ultrasound-imaging-data-for-enhanced-breast-cancer-diagnosis-with-deep-learning-models" class="level3">
<h3 class="anchored" data-anchor-id="organizing-ultrasound-imaging-data-for-enhanced-breast-cancer-diagnosis-with-deep-learning-models">Organizing Ultrasound Imaging Data for Enhanced Breast Cancer Diagnosis with Deep Learning Models</h3>
<p>Pranali Shendekar, May 2023 <a href="./documents/Capstones/Organizing_Ultrasound_Imaging_Data-Pranali_Shendekar.pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> This capstone project aimed to develop a software tool to manage and process ultrasound imaging data to train deep-learning models for predicting the presence of cancerous lesions in breast tissue. The project involved collecting, managing, updating, and summarizing study and annotation data in batches as they become available. The study data included ultrasound images, patient information, biopsy results, BI-RADS scores, and metadata about the images and ultrasound equipment. In addition, the annotation data had additional labels and visual outlines of the lesions, which were provided retrospectively by trained radiologists.</p>
<p>The software tool was designed to extract information from the metadata and text annotations accompanying each image and add it to the master index file. Additionally, the tool had to be able to copy the images to the correct locations in the collection and ensure that the image and patient IDs were distinct from those of previously added data. The tool also allowed for the possibility of overwriting existing data, adding additional columns to the index file, and incorporating corrections. Furthermore, the tool included additional columns that may be collected later from the annotation data.</p>
<p>Python code was also developed to enable simple filtering of the image data and to write data downloaders allowing retrieving the image data from the collection. Finally, to show the efficiency of the database, the dataset was tested on a simple deep learning ResNet50 pre-trained model, and the PyTorch data loaders were used to pass data to the deep learning model.</p>
<p>Overall, this project contributed to developing a tool that can aid in the early detection of breast cancer, leading to better patient outcomes. The software tool developed in this project can be used in further research studies in medical imaging, deep learning, and clinical settings to support radiologists in diagnosing breast cancer.</p>
</section>
<section id="computer-aided-decision-software-for-breast-ultrasound-lesions-business-plan" class="level3">
<h3 class="anchored" data-anchor-id="computer-aided-decision-software-for-breast-ultrasound-lesions-business-plan">Computer-Aided Decision Software for Breast Ultrasound Lesions Business Plan</h3>
<p>Elsa Braun, May 2023 <a href="./documents/Capstones/CAD_for_Breast_Ultrasound-Business_Plan-Elsa_Braun.pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> Our company, Ultrasound.AI, is launching a hybrid deep learning algorithm. that will collaborate with radiologists in order to forecast the presence of breast cancer using ultrasound imagery. Our algorithm is unique in its ability to reduce radiologist time spent reviewing a patient’s images, thus improving throughput. Our algorithm can preview all images in a patient study and prioritize them for the radiologist. The Ultrasound.AI algorithm can match the accuracy of radiologists while reducing the need for unwarranted breast biopsies. Our algorithm uniquely offers transparency to the radiologist. A heatmap process allows the radiologist to measure how the algorithm arrived at its breast cancer prediction. With our next update, the Ultrasound.AI algorithm in collaboration with partner, Visage Imaging, will further save radiologist time by generating standardized diagnosis and treatment data (BI-RADS report) for the patient’s electronic medical record.</p>
</section>
<section id="semantic-segmentation-for-medical-ultrasound-imaging" class="level3">
<h3 class="anchored" data-anchor-id="semantic-segmentation-for-medical-ultrasound-imaging">Semantic Segmentation for Medical Ultrasound Imaging</h3>
<p>Florin Andrei, December 2022, <a href="./documents/Capstones/Semantic Segmentation for Medical Ultrasound Imaging - Florin Andrei.pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> Breast cancer is the type of cancer with the highest prevalence nowadays, around the world. According to the World Health Organization, millions of cases are diagnosed each year, and in the year 2020 it has caused approximately 685,000 deaths worldwide. Forecasts indicate that the number of cases and the number of deaths may increase for the foreseeable future. Within a larger joint project by the University of Wisconsin - La Cross, and the Mayo Clinic Health Systems, this study has developed deep learning techniques that could be used to improve the diagnostic process for breast cancer. There are two main ways that deep learning technology could be applied: by providing real-time visual cues to radiologists while exploring lesions using ultrasound, and by creating predictions that become inputs for other models, in so-called ensemble methods, where multiple models work together to predict characteristics of the lesion. Based on a review of the literature on medical imaging and image segmentation techniques, several semantic segmentation models were created and trained on breast ultrasound imaging datasets. The research found that the best models performed at a level that would be expected from state-of-the-art image segmentation techniques, adjusted for the additional challenges raised by ultrasound imaging. Further work is needed within the parent project to integrate the models trained within this study with software usable by radiologists, and to explore the performance of ensemble methods where multiple models, including models trained in this study, work together to make predictions on ultrasound images that would ultimately lead to better diagnostic results and better patient outcomes.</p>
</section>
</section>
<section id="computer-aided-diagnosis-cad-of-breast-cancer-methods-of-model-explainability" class="level2">
<h2 class="anchored" data-anchor-id="computer-aided-diagnosis-cad-of-breast-cancer-methods-of-model-explainability">Computer-Aided Diagnosis (CAD) of Breast Cancer: Methods of Model Explainability</h2>
<p>Teresa Bodart, December 2022, <a href="./documents/Capstones/Computer-Aided Diagnosis (CAD) of Breast Cancer- Methods of Model Explainability - Teresa Bodart.pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> The International Agency for Research on Cancer announced that in 2020 female breast cancer became the most diagnosed cancer worldwide and the most common cause of cancer-related death in women. Still, breast cancer generally has a good prognosis with timely detection and appropriate treatment. Recently, computer-aided diagnosis (CAD) systems have shown promising results in using artificial intelligence (AI) to detect malignant lesions in breast ultrasound (US) imaging. When working with AI in a clinical setting, however, the American College of Radiology advocates for radiologist understanding of the algorithms in use. Accordingly, this study contributes to an ongoing collaboration between the University of Wisconsin-La Crosse and Mayo Clinic Enterprise by investigating three methods of AI explainability for the CAD software in development. Class activation maps, saliency maps, and attention map-enhanced class activation maps were compared to determine the most useful technique for visualizing regions in the US used by the models to determine pathology. The evaluation showed that saliency maps are the most promising method for visually explaining breast US classification. However, the small dataset and simplified model architecture used in this study mean that further research is necessary before fully implementing this method within the greater collaboration.</p>
<section id="computer-aided-detection-cad-system-for-breast-ultrasound-lesion-interpretation-an-explainable-deep-learning-approach" class="level3">
<h3 class="anchored" data-anchor-id="computer-aided-detection-cad-system-for-breast-ultrasound-lesion-interpretation-an-explainable-deep-learning-approach">Computer-Aided Detection (CAD) System for Breast Ultrasound Lesion Interpretation: An Explainable Deep Learning Approach</h3>
<p>Josh Jarvey, August 2022, <a href="./documents/Capstones/Computer-Aided Detection (CAD) System for Breast Ultrasound Lesion Interpretation - Josh Jarvey.pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> According to the World Health Organization, breast cancer is now the most prevalent type of cancer diagnosed around the world. In 2020, it accounted for the deaths of roughly 685,000 women alone. Breast cancer incidence and mortality rates are expected to continue to rise in the coming decades, primarily in low to middle-income counties, due in part to an adoption of a more western lifestyle coupled with misconceptions about the nature and curability of the disease. Therefore, in partnership between the University of Wisconsin – La Crosse and Mayo Clinic Health Systems, the purpose of this study is to assess the feasibility of utilizing deep learning to aid radiologists with the interpretation of lesions discovered in breast ultrasound (BUS) images during routine clinical screenings. Based on a review of the literature on medical imaging and computer-assisted detection (CAD) systems for BUS interpretation, a multitask learning model using a pre-trained state-of-the-art convolutional neural network (CNN) was developed and trained using various image augmentation techniques known to increase performance. The research found that the best model identified from this study performed on par with that of a trained radiologist in its ability to predict lesion pathology. However, no definitive conclusions could be drawn about the model’s multitask performance due in part to the limited data available. Further research is needed as more data is made available from the Mayo Clinic, and alternative explainability methods may need to be explored.</p>
</section>
<section id="ai-based-breast-cancer-classification-using-ultrasound-images" class="level3">
<h3 class="anchored" data-anchor-id="ai-based-breast-cancer-classification-using-ultrasound-images">AI-Based Breast Cancer Classification Using Ultrasound Images</h3>
<p>Suriya Mohan, May 2022, <a href="./documents/Capstones/AI-Based Breast - Suriya Mohan.pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> Breast cancer is the most frequently diagnosed cancer in women, and it is the leading cause of cancer-related deaths in women. Identifying the tumor in the early stage is essential to increase the survival rate of the patients. The breast ultrasound images combined with biopsy are clinically proven to be an effective method of diagnosing tumors. This project utilizes Artificial Intelligence (AI), such as deep learning models, to segment and classify the tumor in the ultrasound images. A segmentation deep learning model is utilized to detect the tumor in the ultrasound image, identify the region of interest (ROI), and draw a bounding box around the tumor. The radiologist confirms the ROI. Later the ROI is sent to the classification model to categorize cancer as benign or malignant. The radiologist can use the results from these models to make informed decisions, thereby reducing the manual errors involved in the current process. The AI technology assistance and radiologist expertise can lead to a process that is less prone to errors. The models were trained on augmented data set of 452 malignant images and 706 benign images. The accuracy of this model is 76.2%, precision 80%, and F1 score of 72.7%.</p>
</section>
<section id="classification-of-breast-lesions-using-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="classification-of-breast-lesions-using-deep-learning">Classification of Breast Lesions using Deep Learning</h3>
<p>Justin Hall, December 2021. <a href="./documents/Capstones/Classification of Breast Lesions using Deep Learning - Justin Hall .pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> Breast ultrasounds are a standard modality used to find cancerous lesions and provide early care for patients. However, this modality is sensitive to user dependency. The discrepancies between radiologists’ readings for the same image may lead to inadequate patient care and inconsistent outcomes. As breast cancer continues to affect people around the globe, a new approach is needed to detect cancerous lesions. To provide a system for standardized classifications of these images, we examine the use of state-of-the-art convolutional neural network architectures to improve patient care. Our analysis showed that while all models showed similar AUC on our validation dataset of 121 images, DenseNet-201 maximized the F1-score. To provide confidence in our predictions, we then use local interpretable model-agnostic explanations (LIME) to find what image regions are essential to our model. Our model identified areas of lesion consistent with areas identified by trained radiologists suggesting that this model shows promise in aiding radiologists resulting in better care for patients.</p>
</section>
<section id="computer-aided-diagnosis-cad-of-breast-cancer-from-ultrasound-image" class="level3">
<h3 class="anchored" data-anchor-id="computer-aided-diagnosis-cad-of-breast-cancer-from-ultrasound-image">Computer-Aided Diagnosis (CAD) of Breast Cancer from Ultrasound Image</h3>
<p>Adam Silberfein, August 2021.<a href="./documents/Capstones/Computer-Aided Diagnosis (CAD) of Breast Cancer from Ultrasound Images - Adam Silverfein.pdf">Link to pdf file</a></p>
<p><strong>Abstract:</strong> Breast ultrasound has long been one the most preferred modalities of breast cancer detection due to its relatively low cost, ease of access, and noninvasive nature. Advancements in imaging technology and improvements in technique and standardization have led to corresponding increases in accuracy, but the potential for an additional boost exists from the application of artificial intelligence and machine learning. This paper explores how the potential of a quantitative model and associated machine learning prediction can assist radiologists currently using what is primarily a qualitative model to assign a BI-RADS score and characterize breast lesions as benign or malignant. The steps taken involve having an expert radiologist annotate a publicly available data set of 135 breast ultrasound images according to an established rubric, using image processing techniques to convert certain highly predictive features of that rubric to a numerical representation, and then feeding that quantitative representation into a machine learning model to predict malignancy of a validation set of images. The resulting model confirms that not only were lesion shape and circumscription important features for classification, but so was internal echotexture, which is much more difficult to differentiate to the naked eye. The model will be used to classify future larger data sets provided by our client, either in its current manifestation or after retraining on a subset of those new images.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>